# -*- coding: utf-8 -*-
"""Traffic Sign Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QD4LdKLCgplaa0e_Zybc6qtFM4NwEDeA
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from tensorflow.keras import layers, models, callbacks
from google.colab import drive, files

# --- CONFIGURATION ---
DATASET_PATH = '/content/drive/MyDrive/datasets'

IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32
INITIAL_EPOCHS = 20
FINE_TUNE_EPOCHS = 20

def mount_drive():
    if not os.path.exists('/content/drive'):
        print("Mounting Google Drive...")
        drive.mount('/content/drive')
    else:
        print("Drive is already mounted.")

def get_class_weights(data_dir):
    print("\nCalculating Class Weights to balance training...")
    classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
    counts = []
    for c in classes:
        class_path = os.path.join(data_dir, c)
        count = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])
        counts.append(count)

    total = sum(counts)
    if total == 0: return None

    weights = {}
    for i, count in enumerate(counts):
        if count > 0:
            weights[i] = total / (len(classes) * count)
        else:
            weights[i] = 1.0

    print(f"   -> Found {total} images across {len(classes)} classes.")
    print(f"   -> Weights applied.")
    return weights

def load_custom_data(data_dir):
    if not os.path.exists(data_dir):
        print(f"ERROR: Path not found: {data_dir}")
        return None, None, 0, [], None

    print(f"Checking folder: {data_dir}")
    subfolders = [f.name for f in os.scandir(data_dir) if f.is_dir()]
    if len(subfolders) == 1:
        print(f"Diving inside '{subfolders[0]}'...")
        data_dir = os.path.join(data_dir, subfolders[0])
        subfolders = [f.name for f in os.scandir(data_dir) if f.is_dir()]

    num_classes = len(subfolders)
    print(f"Found {num_classes} classes.")

    if num_classes < 2:
        print("\nERROR: Need at least 2 classes.")
        return None, None, 0, [], None

    class_weights = get_class_weights(data_dir)

    print("Loading images...")
    # Training Set
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=(IMG_HEIGHT, IMG_WIDTH),
        batch_size=BATCH_SIZE,
        label_mode='categorical'
    )

    # Validation Set
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=(IMG_HEIGHT, IMG_WIDTH),
        batch_size=BATCH_SIZE,
        label_mode='categorical'
    )

    class_names = train_ds.class_names

    # Optimization
    train_ds = train_ds.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)
    val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)

    return train_ds, val_ds, num_classes, class_names, class_weights

def create_model(num_classes):
    print("Downloading EfficientNetV2B0 (High Accuracy)...")

    data_augmentation = tf.keras.Sequential([
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1),
        layers.RandomContrast(0.1),
        layers.RandomTranslation(0.1, 0.1),
        layers.RandomBrightness(0.1),
    ])

    base_model = tf.keras.applications.EfficientNetV2B0(
        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),
        include_top=False,
        weights='imagenet'
    )
    base_model.trainable = False

    inputs = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))
    x = data_augmentation(inputs)
    x = base_model(x, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)

    model.compile(optimizer='adam',
                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                  metrics=['accuracy'])
    return model, base_model

# --- MAIN EXECUTION ---

mount_drive()
train_ds, val_ds, num_classes, class_names, class_weights = load_custom_data(DATASET_PATH)

if train_ds and num_classes > 1:
    print(f"\nBuilding EfficientNetV2 model for {num_classes} classes...")
    model, base_model = create_model(num_classes)

    # --- PHASE 1: TRAIN HEAD ---
    print(f"\nPhase 1: Training top layers ({INITIAL_EPOCHS} epochs)...")
    history = model.fit(
        train_ds,
        epochs=INITIAL_EPOCHS,
        validation_data=val_ds,
        class_weight=class_weights
    )

    # --- PHASE 2: FINE-TUNING ---
    print(f"\nPhase 2: Fine-tuning base model ({FINE_TUNE_EPOCHS} epochs)...")

    base_model.trainable = True
    for layer in base_model.layers[:100]:
        layer.trainable = False

    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                  optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5),
                  metrics=['accuracy'])

    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)
    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7)

    history_fine = model.fit(
        train_ds,
        epochs=FINE_TUNE_EPOCHS + INITIAL_EPOCHS,
        initial_epoch=history.epoch[-1],
        validation_data=val_ds,
        callbacks=[early_stop, reduce_lr],
        class_weight=class_weights
    )

    model.save('traffic_model_efficient.keras')
    print("\nModel saved successfully as 'traffic_model_efficient.keras'!")

    # --- DETAILED ACCURACY REPORT ---
    print("\n Generating Overall Accuracy Report...")

    y_true = []
    y_pred = []

    print("Processing validation set (this might take a moment)...")
    for images, labels in val_ds:
        preds = model.predict(images, verbose=0)
        y_true.extend(np.argmax(labels.numpy(), axis=1))
        y_pred.extend(np.argmax(preds, axis=1))

    # 2. Calculate metrics
    overall_acc = accuracy_score(y_true, y_pred)
    print(f"\nOverall Model Accuracy: {overall_acc * 100:.2f}%")

    print("\n--- Detailed Classification Report ---")

    # FIX: We now explicitly define 'labels' to handle missing classes in validation set
    print(classification_report(
        y_true,
        y_pred,
        target_names=class_names,
        labels=np.arange(len(class_names)),
        zero_division=0
    ))

    # Plot graphs
    acc = history.history['accuracy'] + history_fine.history['accuracy']
    val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']
    plt.figure(figsize=(8, 8))
    plt.plot(acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.plot([INITIAL_EPOCHS-1,INITIAL_EPOCHS-1], plt.ylim(), label='Start Fine Tuning')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')
    plt.show()

def upload_and_predict():
    print("Upload an image to test:")
    uploaded = files.upload()
    for fn in uploaded.keys():
        img = cv2.imread(fn)
        if img is None: continue

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (IMG_HEIGHT, IMG_WIDTH))

        img_array = np.expand_dims(img_resized, axis=0)

        pred = model.predict(img_array)
        idx = np.argmax(pred)
        confidence = np.max(pred)

        print(f"Prediction: {class_names[idx]} ({confidence*100:.2f}%)")
        plt.imshow(img_rgb)
        plt.axis('off')
        plt.show()
upload_and_predict()